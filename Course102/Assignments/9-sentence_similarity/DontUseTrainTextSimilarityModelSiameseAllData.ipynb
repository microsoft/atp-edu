{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bb94d6d-23cc-4a75-8707-4e0eaa629d00",
   "metadata": {},
   "source": [
    "# 基于Bert训练一个文本相似度模型\n",
    "\n",
    "bert+maxpooling +cosine 计算相似度，只使用正例进行训练（约占训练集1/3 1w条数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "707d7f02-137e-457c-87d8-c0efe9f93706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use gpu1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,TFAutoModel,AutoConfig,TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus)>1:\n",
    "    tf.config.set_visible_devices(gpus[1], 'GPU')\n",
    "    print('use gpu1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0338c18-ae83-4064-90ae-5f701fdff849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 10:35:44.900622: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-14 10:35:45.454955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6659 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:b3:00.0, compute capability: 7.5\n",
      "Some layers from the model checkpoint at bert-base-chinese were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# config = AutoConfig.from_pretrained('bert-base-chinese')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = TFAutoModel.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c1e4a-1de1-4a1f-bc5f-fdb8b3aa5101",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 构造数据集\n",
    "\n",
    "### 这里使用 AFQMC 蚂蚁金融语义相似度 Ant Financial Question Matching Corpus， 但只使用其中的正例模拟常见应用场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34ee54cf-8e11-4cca-8564-f34e8e3a7908",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pairs=[]\n",
    "with open('train.json') as f:\n",
    "    for l in f.readlines():\n",
    "        data = json.loads(l)\n",
    "        if data['label'] == '1':\n",
    "            sentence_pairs.append((data['sentence1'],data['sentence2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8cbf9e4-f79b-4bf2-bc86-266d4047e3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据对数： 10573\n",
      "训练数据平均长度： 13.40915539581954\n",
      "训练数据最大长度： 90\n"
     ]
    }
   ],
   "source": [
    "print('训练数据对数：',len(sentence_pairs))\n",
    "print('训练数据平均长度：', sum([len(s1)+len(s2) for s1,s2 in sentence_pairs])/(len(sentence_pairs)*2))\n",
    "print('训练数据最大长度：', max([max(len(s1),len(s2)) for s1,s2 in sentence_pairs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "855eeac7-c10f-4b4e-bb15-4ac7d8d0c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9087c1c-bb80-4cad-a9cb-4a4dae22e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "s0_encoded = tokenizer([p[0] for p in sentence_pairs], padding=True, truncation=True, max_length=MAX_LEN,return_tensors=\"tf\")\n",
    "s1_encoded = tokenizer([p[1] for p in sentence_pairs], padding=True, truncation=True, max_length=MAX_LEN,return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af8fa187-652e-4278-82bb-047678a54afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取dev数据\n",
    "dev_sentence_pairs=[]\n",
    "dev_labels=[]\n",
    "with open('dev.json') as f:\n",
    "    for l in f.readlines():\n",
    "        data = json.loads(l)\n",
    "        dev_sentence_pairs.append((data['sentence1'],data['sentence2']))\n",
    "        dev_labels.append(True if data['label']=='1' else False)\n",
    "dev_s0_encoded = tokenizer([p[0] for p in dev_sentence_pairs], padding=True, truncation=True, max_length=MAX_LEN,return_tensors=\"tf\")\n",
    "dev_s1_encoded = tokenizer([p[1] for p in dev_sentence_pairs], padding=True, truncation=True, max_length=MAX_LEN,return_tensors=\"tf\")\n",
    "dev_s1_input_ids = dev_s0_encoded['input_ids']\n",
    "dev_s1_token_type_ids = dev_s0_encoded['token_type_ids']\n",
    "dev_s1_attention_mask = dev_s0_encoded['attention_mask']\n",
    "dev_s2_input_ids = dev_s1_encoded['input_ids']\n",
    "dev_s2_token_type_ids = dev_s1_encoded['token_type_ids']\n",
    "dev_s2_attention_mask = dev_s1_encoded['attention_mask']\n",
    "dev_labels = tf.constant(dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1508d46f-5077-4ed2-b096-4d0965dda88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(4316, 50), dtype=int32, numpy=\n",
       "array([[ 101, 1352, 1282, ...,    0,    0,    0],\n",
       "       [ 101, 5709, 1446, ...,    0,    0,    0],\n",
       "       [ 101, 2769, 4638, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 101, 2582, 3416, ...,    0,    0,    0],\n",
       "       [ 101, 5709, 1446, ...,    0,    0,    0],\n",
       "       [ 101, 2769, 4638, ...,    0,    0,    0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(4316, 50), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(4316, 50), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_s0_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a063c67-986a-4b36-b817-d03dea2bf7dd",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55f6c2bf-f4a7-4b59-acbf-1d63593cee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.trainable = False\n",
    "\n",
    "@tf.function\n",
    "def max_pooling_with_mask(paras):\n",
    "    sent_word_embeddings,sent_indices = paras\n",
    "    not_padding = tf.math.not_equal(sent_indices,0)\n",
    "    not_cls = tf.math.not_equal(sent_indices,101) # cls\n",
    "    not_seg = tf.math.not_equal(sent_indices,102) # sep\n",
    "    mask = tf.math.logical_and(not_padding,not_cls)\n",
    "    mask = tf.math.logical_and(mask,not_seg)\n",
    "    mask_f = tf.cast(mask,tf.float32)\n",
    "    mask_f = tf.expand_dims(mask_f,axis=-1)\n",
    "    return tf.reduce_max(tf.multiply(sent_word_embeddings,mask_f),axis=1)\n",
    "\n",
    "def convert_bert_to_sentence_embedding_model(bert_model,sentence_embedding_size=256):\n",
    "    input_ids = keras.Input(shape=(None,),dtype=tf.int32)\n",
    "    token_type_ids = keras.Input(shape=(None,),dtype=tf.int32)\n",
    "    attention_mask = keras.Input(shape=(None,),dtype=tf.int32)\n",
    "    output = bert_model(input_ids,token_type_ids,attention_mask)\n",
    "    bert_embeddings = output.last_hidden_state\n",
    "    sentence_embeddings = keras.layers.Lambda(max_pooling_with_mask,name='lambda_max_pooling')([bert_embeddings,input_ids]) # 这里使用max_pooling作为句子的embedding\n",
    "    # sentence_embeddings = keras.layers.Dense(sentence_embedding_size,name='dense_layer',activation='relu')(sentence_embeddings)\n",
    "    normalized_sentence_embeddings = keras.layers.Lambda(lambda xt: tf.nn.l2_normalize(xt,axis=1))(sentence_embeddings)\n",
    "\n",
    "    return keras.Model([input_ids,token_type_ids,attention_mask],normalized_sentence_embeddings,name='sentence_embedding_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f449dfe-e18e-44a4-85bc-0472bf8a66e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding_model = convert_bert_to_sentence_embedding_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac1a9816-ff23-44f8-80a7-85d0a552681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_siamese_model(sentence_embedding_model):\n",
    "    s1_input_ids = keras.Input(shape=(None,),dtype=tf.int32)\n",
    "    s1_token_type_ids = keras.Input(shape=(None,),dtype=tf.int32)\n",
    "    s1_attention_mask = keras.Input(shape=(None,),dtype=tf.int32)\n",
    "    s2_input_ids = keras.Input(shape=(None,),dtype=tf.int32)\n",
    "    s2_token_type_ids = keras.Input(shape=(None,),dtype=tf.int32)\n",
    "    s2_attention_mask = keras.Input(shape=(None,),dtype=tf.int32)\n",
    "\n",
    "    s1_embeddings = sentence_embedding_model([s1_input_ids,s1_token_type_ids,s1_attention_mask])\n",
    "    s2_embeddings = sentence_embedding_model([s2_input_ids,s2_token_type_ids,s2_attention_mask]) \n",
    "    stack_embeddings = keras.layers.Lambda(lambda x: tf.stack(x,axis=1))([s1_embeddings,s2_embeddings])\n",
    "    return keras.Model([s1_input_ids,s1_token_type_ids,s1_attention_mask,s2_input_ids,s2_token_type_ids,s2_attention_mask],stack_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96539b58-01ef-430d-a592-aad497c26e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = create_siamese_model(sentence_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbaacaea-db9e-496a-b0cc-a08f5af0a8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " sentence_embedding_model (Func  (None, 768)         102267648   ['input_4[0][0]',                \n",
      " tional)                                                          'input_5[0][0]',                \n",
      "                                                                  'input_6[0][0]',                \n",
      "                                                                  'input_7[0][0]',                \n",
      "                                                                  'input_8[0][0]',                \n",
      "                                                                  'input_9[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 2, 768)       0           ['sentence_embedding_model[0][0]'\n",
      "                                                                 , 'sentence_embedding_model[1][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 102,267,648\n",
      "Trainable params: 102,267,648\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf5387b7-b860-45dc-8bd2-04d82f85b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def negative_ranking_loss_func(_, embeddings):\n",
    "    query_embeddings,question_embeddings = embeddings[:,0],embeddings[:,1]\n",
    "\n",
    "    score_matrix = tf.linalg.matmul(query_embeddings,tf.transpose(question_embeddings))\n",
    "    labels = tf.one_hot(tf.range(len(embeddings)), len(embeddings))\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels,score_matrix*10)\n",
    "    return loss\n",
    "\n",
    "adam_opt = keras.optimizers.Adam(learning_rate=2e-5)\n",
    "siamese_model.compile(loss=negative_ranking_loss_func, optimizer=adam_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff7a6d-5092-4004-b136-89b50c7e889f",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f11ead7-4f09-4505-9206-6123e8a62e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_input_ids = s0_encoded['input_ids']\n",
    "s1_token_type_ids = s0_encoded['token_type_ids']\n",
    "s1_attention_mask = s0_encoded['attention_mask']\n",
    "s2_input_ids = s1_encoded['input_ids']\n",
    "s2_token_type_ids = s1_encoded['token_type_ids']\n",
    "s2_attention_mask = s1_encoded['attention_mask']\n",
    "y_placeholder = tf.ones(len(s1_input_ids),dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c428bfe-0f33-405d-bfc6-568cb1e181d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy_manually(model,data_group):\n",
    "    dev_s1_input_ids,dev_s1_token_type_ids,dev_s1_attention_mask,dev_s2_input_ids,dev_s2_token_type_ids,dev_s2_attention_mask,dev_labels = data_group\n",
    "        \n",
    "    embeddings = np.asarray(model.predict([dev_s1_input_ids,dev_s1_token_type_ids,dev_s1_attention_mask,dev_s2_input_ids,dev_s2_token_type_ids,dev_s2_attention_mask]))\n",
    "    queries_embeddings = embeddings[:,0]\n",
    "    questions_embeddings = embeddings[:,1]\n",
    "    scores = tf.linalg.matmul(queries_embeddings,tf.transpose(questions_embeddings))\n",
    "    scores = tf.linalg.diag_part(scores)\n",
    "    predict_labels = tf.math.greater(scores,0.5)\n",
    "    return tf.math.reduce_mean(tf.cast(tf.math.equal(predict_labels,dev_labels),tf.float32))\n",
    "    \n",
    "\n",
    "class CalcAccuracyCallback(keras.callbacks.Callback): \n",
    "    def __init__(self,val_data):\n",
    "        super(keras.callbacks.Callback, self).__init__()\n",
    "        self.val_data = val_data \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        accuracy = check_accuracy_manually(self.model,self.val_data)\n",
    "        tf.print(\"accuracy on dev set is \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbfb2a9-b697-423a-8362-48c33f0aac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.fit([s1_input_ids,s1_token_type_ids,s1_attention_mask,s2_input_ids,s2_token_type_ids,s2_attention_mask],y_placeholder,\n",
    "                  # validation_data = ([dev_s1_input_ids,dev_s1_token_type_ids,dev_s1_attention_mask,dev_s2_input_ids,dev_s2_token_type_ids,dev_s2_attention_mask],dev_labels),\n",
    "                  callbacks=[CalcAccuracyCallback((s1_input_ids[:1000],s1_token_type_ids[:1000],s1_attention_mask[:1000],s2_input_ids[:1000],s2_token_type_ids[:1000],s2_attention_mask[:1000],y_placeholder[:1000])),\n",
    "                            CalcAccuracyCallback((dev_s1_input_ids,dev_s1_token_type_ids,dev_s1_attention_mask,dev_s2_input_ids,dev_s2_token_type_ids,dev_s2_attention_mask,dev_labels))],\n",
    "                  epochs=50,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642080da-435e-43e0-b4be-1b7a367d9cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# siamese_model.fit([s1_input_ids,s1_token_type_ids,s1_attention_mask,s2_input_ids,s2_token_type_ids,s2_attention_mask],y_placeholder,\n",
    "#                   validation_data = ([dev_s1_input_ids,dev_s1_token_type_ids,dev_s1_attention_mask,dev_s2_input_ids,dev_s2_token_type_ids,dev_s2_attention_mask],dev_labels),\n",
    "#                   callbacks=[CalcAccuracyCallback((dev_s1_input_ids,dev_s1_token_type_ids,dev_s1_attention_mask,dev_s2_input_ids,dev_s2_token_type_ids,dev_s2_attention_mask,dev_labels))],\n",
    "#                   epochs=100,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8cadf5-5240-474e-855b-09f79ac44799",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"今天下午可能会下雨\"\n",
    "sentence2= '今天天气很晴朗'\n",
    "sentence3= '天气预报说下午有雨'\n",
    "sentence4= '北京是中国的首都'\n",
    "\n",
    "sentence_list = [sentence1,sentence2,sentence3,sentence4]\n",
    "encoded_inputs = tokenizer(sentence_list,return_tensors='tf',padding=True)\n",
    "sentence_embeddins = sentence_embedding_model(encoded_inputs)\n",
    "for i,s1 in enumerate(sentence_list):\n",
    "    for j,s2 in enumerate(sentence_list):\n",
    "        print(f'[{s1}]和[{s2}]的相似度为：',np.matmul(sentence_embeddins[i], tf.transpose(sentence_embeddins[j])))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88135a6-af74-4392-a113-3b41e0a82b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (s1,s2),label in zip(dev_sentence_pairs,dev_labels):\n",
    "    s1_embedding = sentence_embedding_model(tokenizer(s1,return_tensors='tf',padding=True))[0]\n",
    "    s2_embedding = sentence_embedding_model(tokenizer(s2,return_tensors='tf',padding=True))[0]\n",
    "    print(f'[{label}][{s1}]和[{s2}]的相似度为：',np.matmul(s1_embedding, s2_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997d4b0-0c6d-4763-ad6a-6dcdc5622dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
