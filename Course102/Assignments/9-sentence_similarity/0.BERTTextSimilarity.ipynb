{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3efddb53-8979-45da-8a6d-14fb6225d9a1",
   "metadata": {},
   "source": [
    "# 1. 直接使用Bert+mean pooling+ l2 距离 进行相似度计算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db7a033-5a58-47b5-aff7-7a845e4f0191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,TFAutoModel,AutoConfig\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da150df6-7e03-4649-82bb-cb9466cf503d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 16:27:40.191371: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-16 16:27:47.058212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5496 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:17:00.0, compute capability: 7.5\n",
      "2022-03-16 16:27:47.058937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 5513 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:b3:00.0, compute capability: 7.5\n",
      "Some layers from the model checkpoint at bert-base-chinese were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# config = AutoConfig.from_pretrained('bert-base-chinese')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = TFAutoModel.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1c1a96b-8b97-46fd-93d5-18e3fd83e21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(4, 11), dtype=int32, numpy=\n",
       "array([[ 101,  791, 1921,  678, 1286, 1377, 5543,  833,  678, 7433,  102],\n",
       "       [ 101,  791, 1921, 1921, 3698, 2523, 3252, 3306,  102,    0,    0],\n",
       "       [ 101, 1921, 3698, 7564, 2845, 6432,  678, 1286, 3300, 7433,  102],\n",
       "       [ 101, 1266,  776, 3221,  704, 1744, 4638, 7674, 6963,  102,    0]],\n",
       "      dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(4, 11), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(4, 11), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = \"今天下午可能会下雨\"\n",
    "sentence2= '今天天气很晴朗'\n",
    "sentence3= '天气预报说下午有雨'\n",
    "sentence4= '北京是中国的首都'\n",
    "\n",
    "sentence_list = [sentence1,sentence2,sentence3,sentence4]\n",
    "encoded_inputs = tokenizer(sentence_list,return_tensors='tf',padding=True)\n",
    "encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c904da0b-9eb6-4d1c-b4aa-24d4f03419e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[今天下午可能会下雨]和[今天下午可能会下雨]的距离为： 0.0\n",
      "[今天下午可能会下雨]和[今天天气很晴朗]的距离为： 97.238594\n",
      "[今天下午可能会下雨]和[天气预报说下午有雨]的距离为： 54.66022\n",
      "[今天下午可能会下雨]和[北京是中国的首都]的距离为： 185.97879\n",
      "[今天天气很晴朗]和[今天下午可能会下雨]的距离为： 97.238594\n",
      "[今天天气很晴朗]和[今天天气很晴朗]的距离为： 0.0\n",
      "[今天天气很晴朗]和[天气预报说下午有雨]的距离为： 91.689384\n",
      "[今天天气很晴朗]和[北京是中国的首都]的距离为： 199.58101\n",
      "[天气预报说下午有雨]和[今天下午可能会下雨]的距离为： 54.66022\n",
      "[天气预报说下午有雨]和[今天天气很晴朗]的距离为： 91.689384\n",
      "[天气预报说下午有雨]和[天气预报说下午有雨]的距离为： 0.0\n",
      "[天气预报说下午有雨]和[北京是中国的首都]的距离为： 184.15388\n",
      "[北京是中国的首都]和[今天下午可能会下雨]的距离为： 185.97879\n",
      "[北京是中国的首都]和[今天天气很晴朗]的距离为： 199.58101\n",
      "[北京是中国的首都]和[天气预报说下午有雨]的距离为： 184.15388\n",
      "[北京是中国的首都]和[北京是中国的首都]的距离为： 0.0\n"
     ]
    }
   ],
   "source": [
    "sentence_embeddins = tf.math.reduce_mean(model(encoded_inputs).last_hidden_state,axis=1)\n",
    "\n",
    "for i,s1 in enumerate(sentence_list):\n",
    "    for j,s2 in enumerate(sentence_list):\n",
    "        print(f'[{s1}]和[{s2}]的距离为：',tf.math.reduce_sum(tf.math.square(sentence_embeddins[i]-sentence_embeddins[j])).numpy())\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ad047-a2a2-483a-b368-a05c56b4f23d",
   "metadata": {},
   "source": [
    "# 2. 使用sentencebert+mean pooling + cosine距离 进行相似度计算\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6e1af46-7a76-407a-8aad-c3fa51daae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "#Mean Pooling - Take average of all tokens\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "#Encode text\n",
    "def encode(texts):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7a68329-522c-4bff-9795-d7b1771baaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[今天下午可能会下雨]和[今天下午可能会下雨]的相似度为： 1.0\n",
      "[今天下午可能会下雨]和[今天天气很晴朗]的相似度为： 0.5500054\n",
      "[今天下午可能会下雨]和[天气预报说下午有雨]的相似度为： 0.79448617\n",
      "[今天下午可能会下雨]和[北京是中国的首都]的相似度为： 0.30826226\n",
      "[今天天气很晴朗]和[今天下午可能会下雨]的相似度为： 0.5500054\n",
      "[今天天气很晴朗]和[今天天气很晴朗]的相似度为： 0.9999999\n",
      "[今天天气很晴朗]和[天气预报说下午有雨]的相似度为： 0.7026714\n",
      "[今天天气很晴朗]和[北京是中国的首都]的相似度为： 0.21765988\n",
      "[天气预报说下午有雨]和[今天下午可能会下雨]的相似度为： 0.79448617\n",
      "[天气预报说下午有雨]和[今天天气很晴朗]的相似度为： 0.7026714\n",
      "[天气预报说下午有雨]和[天气预报说下午有雨]的相似度为： 1.0000002\n",
      "[天气预报说下午有雨]和[北京是中国的首都]的相似度为： 0.27188224\n",
      "[北京是中国的首都]和[今天下午可能会下雨]的相似度为： 0.30826226\n",
      "[北京是中国的首都]和[今天天气很晴朗]的相似度为： 0.21765988\n",
      "[北京是中国的首都]和[天气预报说下午有雨]的相似度为： 0.27188224\n",
      "[北京是中国的首都]和[北京是中国的首都]的相似度为： 1.0000002\n"
     ]
    }
   ],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "query = \"How many people live in London?\"\n",
    "docs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n",
    "\n",
    "\n",
    "#Encode query and docs\n",
    "query_emb = encode(sentence_list)\n",
    "doc_emb = encode(sentence_list)\n",
    "\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = torch.mm(query_emb, doc_emb.transpose(0, 1)).cpu()\n",
    "\n",
    "for i,s1 in enumerate(sentence_list):\n",
    "    for j,s2 in enumerate(sentence_list):\n",
    "        print(f'[{s1}]和[{s2}]的相似度为：',scores[i,j].numpy())\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e2ed7d-4204-4875-9390-7e73f610dc08",
   "metadata": {},
   "source": [
    "# 分析阿里的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e36656d7-28e7-49a3-91d2-a9faa46b55f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc5870b3-4476-41c2-be65-5fa069348e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_pairs=[]\n",
    "s2_pairs=[]\n",
    "labels=[]\n",
    "with open('train.json') as f:\n",
    "    for l in f.readlines():\n",
    "        data = json.loads(l)\n",
    "        s1_pairs.append(data['sentence1'])\n",
    "        s2_pairs.append(data['sentence2'])\n",
    "        labels.append(True if data['label']=='1' else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e593bec0-c31d-40ce-982c-20de31276b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集大小： 34334\n",
      "正例数：10573,占比 0.3079454767868585\n",
      "负例数：23761,占比 0.6920545232131415\n"
     ]
    }
   ],
   "source": [
    "# 阿里数据集基本信息\n",
    "print('训练数据集大小：',len(labels))\n",
    "print(f'正例数：{sum(labels)},占比 {sum(labels)/len(labels)}')\n",
    "print(f'负例数：{sum([not l for l in labels])},占比 {sum([not l for l in labels])/len(labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36aa579b-f05e-433d-a0c6-097beab2f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_s1_pairs=[]\n",
    "dev_s2_pairs=[]\n",
    "dev_labels=[]\n",
    "with open('dev.json') as f:\n",
    "    for l in f.readlines():\n",
    "        data = json.loads(l)\n",
    "        dev_s1_pairs.append(data['sentence1'])\n",
    "        dev_s2_pairs.append(data['sentence2'])\n",
    "        dev_labels.append(True if data['label']=='1' else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "877e3237-4fd6-4a27-83f7-661bcd118cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试数据集大小： 4316\n",
      "正例数：1338,占比 0.31000926784059313\n",
      "负例数：2978,占比 0.6899907321594069\n"
     ]
    }
   ],
   "source": [
    "# 阿里数据集基本信息\n",
    "print('测试数据集大小：',len(dev_labels))\n",
    "print(f'正例数：{sum(dev_labels)},占比 {sum(dev_labels)/len(dev_labels)}')\n",
    "print(f'负例数：{sum([not l for l in dev_labels])},占比 {sum([not l for l in dev_labels])/len(dev_labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6c5415-e965-4147-a62b-d9517c6b5679",
   "metadata": {},
   "source": [
    "# 模型效果评估-测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85671f3d-e56c-4edc-a5d1-cdfc6cd06a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_s1_embs = encode(dev_s1_pairs)\n",
    "dev_s2_embs = encode(dev_s2_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5529ef84-11bc-4d3f-bd20-66cae02bcff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.diag(torch.mm(dev_s1_embs,dev_s2_embs.transpose(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cae292dd-5bbb-4bea-813a-25c44edd8bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4316it [00:00, 144188.80it/s]\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "tp=0\n",
    "tn=0\n",
    "fp=0\n",
    "fn=0\n",
    "for score,label in tqdm.tqdm(zip(scores,dev_labels)):\n",
    "    pred = True if  score>0.5 else False\n",
    "    if label == pred:\n",
    "        correct+=1\n",
    "        if label :\n",
    "            tp+=1\n",
    "        else:\n",
    "            tn+=1\n",
    "    else:\n",
    "        if pred :\n",
    "            fp+=1\n",
    "        else:\n",
    "            fn+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a361cbe-21d4-449f-8802-454b0fda727b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集正确率 0.3561167747914736\n",
      "TP:1256, TN:281, FP:2697, FN:82\n"
     ]
    }
   ],
   "source": [
    "print('测试集正确率',correct/len(dev_labels))\n",
    "print(f'TP:{tp}, TN:{tn}, FP:{fp}, FN:{fn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ab18a-834c-427d-9012-88e10b876181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
