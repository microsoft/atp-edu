{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bb94d6d-23cc-4a75-8707-4e0eaa629d00",
   "metadata": {},
   "source": [
    "# 基于Bert训练一个文本相似度模型\n",
    "1. 使用分类方法训练句子嵌入,使用所有正负例数据\n",
    "2. 句子嵌入采用meanpooling +normalize\n",
    "3. 分类用 cosing+sigmoid,embedding结果更具有解释性，计算相似度时可以调整句子顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "707d7f02-137e-457c-87d8-c0efe9f93706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use gpu1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,TFAutoModel,AutoConfig,TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus)>1:\n",
    "    tf.config.set_visible_devices(gpus[1], 'GPU')\n",
    "    print('use gpu1')\n",
    "    \n",
    "RANDOM_SEED=68\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0338c18-ae83-4064-90ae-5f701fdff849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 17:12:09.462326: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-16 17:12:13.566405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4098 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:b3:00.0, compute capability: 7.5\n",
      "Some layers from the model checkpoint at bert-base-chinese were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# config = AutoConfig.from_pretrained('bert-base-chinese')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = TFAutoModel.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c1e4a-1de1-4a1f-bc5f-fdb8b3aa5101",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 构造数据集\n",
    "\n",
    "### 这里使用 AFQMC 蚂蚁金融语义相似度 Ant Financial Question Matching Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34ee54cf-8e11-4cca-8564-f34e8e3a7908",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pairs=[]\n",
    "labels=[]\n",
    "with open('train.json') as f:\n",
    "    for l in f.readlines():\n",
    "        data = json.loads(l)\n",
    "        labels.append(True if data['label']=='1' else False)\n",
    "        sentence_pairs.append((data['sentence1'],data['sentence2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8cbf9e4-f79b-4bf2-bc86-266d4047e3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据对数： 34334\n",
      "训练数据平均长度： 13.366298712646357\n",
      "训练数据最大长度： 112\n"
     ]
    }
   ],
   "source": [
    "print('训练数据对数：',len(sentence_pairs))\n",
    "print('训练数据平均长度：', sum([len(s1)+len(s2) for s1,s2 in sentence_pairs])/(len(sentence_pairs)*2))\n",
    "print('训练数据最大长度：', max([max(len(s1),len(s2)) for s1,s2 in sentence_pairs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "855eeac7-c10f-4b4e-bb15-4ac7d8d0c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9087c1c-bb80-4cad-a9cb-4a4dae22e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "s0_encoded = tokenizer([p[0] for p in sentence_pairs], padding=True, truncation=True, max_length=MAX_LEN,return_tensors=\"tf\")\n",
    "s1_encoded = tokenizer([p[1] for p in sentence_pairs], padding=True, truncation=True, max_length=MAX_LEN,return_tensors=\"tf\")\n",
    "labels = tf.reshape(tf.constant(labels),shape=(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af8fa187-652e-4278-82bb-047678a54afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取dev数据\n",
    "dev_sentence_pairs=[]\n",
    "dev_labels=[]\n",
    "with open('dev.json') as f:\n",
    "    for l in f.readlines():\n",
    "        data = json.loads(l)\n",
    "        dev_sentence_pairs.append((data['sentence1'],data['sentence2']))\n",
    "        dev_labels.append(True if data['label']=='1' else False)\n",
    "dev_s0_encoded = tokenizer([p[0] for p in dev_sentence_pairs], padding=True, truncation=True, max_length=MAX_LEN,return_tensors=\"tf\")\n",
    "dev_s1_encoded = tokenizer([p[1] for p in dev_sentence_pairs], padding=True, truncation=True, max_length=MAX_LEN,return_tensors=\"tf\")\n",
    "dev_s1_input_ids = dev_s0_encoded['input_ids']\n",
    "dev_s1_token_type_ids = dev_s0_encoded['token_type_ids']\n",
    "dev_s1_attention_mask = dev_s0_encoded['attention_mask']\n",
    "dev_s2_input_ids = dev_s1_encoded['input_ids']\n",
    "dev_s2_token_type_ids = dev_s1_encoded['token_type_ids']\n",
    "dev_s2_attention_mask = dev_s1_encoded['attention_mask']\n",
    "dev_labels = tf.reshape(tf.constant(dev_labels),shape=(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a063c67-986a-4b36-b817-d03dea2bf7dd",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55f6c2bf-f4a7-4b59-acbf-1d63593cee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def mean_pooling_with_mask(paras):\n",
    "    sent_word_embeddings,sent_indices = paras\n",
    "    not_padding = tf.math.not_equal(sent_indices,0)\n",
    "    not_cls = tf.math.not_equal(sent_indices,101) # cls\n",
    "    not_seg = tf.math.not_equal(sent_indices,102) # sep\n",
    "    mask = tf.math.logical_and(not_padding,not_cls)\n",
    "    mask = tf.math.logical_and(mask,not_seg)\n",
    "    mask_f = tf.cast(mask,tf.float32)\n",
    "    mask_f = tf.expand_dims(mask_f,axis=-1)\n",
    "    return tf.reduce_mean(tf.multiply(sent_word_embeddings,mask_f),axis=1)\n",
    "\n",
    "\n",
    "def convert_bert_to_sentence_embedding_model(bert_model):\n",
    "    input_ids = keras.Input(shape=(None,),dtype=tf.int32)\n",
    "    token_type_ids = keras.Input(shape=(None,),dtype=tf.int32)\n",
    "    attention_mask = keras.Input(shape=(None,),dtype=tf.int32)\n",
    "    output = bert_model(input_ids,attention_mask,token_type_ids) # 输入顺序需要和TFBERTModel方法的参数顺序对应\n",
    "    bert_embeddings = output.last_hidden_state\n",
    "    sentence_embeddings = keras.layers.Lambda(mean_pooling_with_mask,name='lambda_mean_pooling')([bert_embeddings,input_ids]) # 这里使用max_pooling作为句子的embedding\n",
    "    normalized_sentence_embeddings = keras.layers.Lambda(lambda xt: tf.nn.l2_normalize(xt,axis=1))(sentence_embeddings)\n",
    "\n",
    "    return keras.Model([input_ids,token_type_ids,attention_mask],sentence_embeddings,name='sentence_embedding_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f449dfe-e18e-44a4-85bc-0472bf8a66e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding_model = convert_bert_to_sentence_embedding_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac1a9816-ff23-44f8-80a7-85d0a552681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里用dense层，也可以考虑计算cos距离\n",
    "# @tf.function\n",
    "def create_siamese_model(sentence_embedding_model):\n",
    "    s1_input_ids = keras.Input(shape=(None,),dtype=tf.int32)\n",
    "    s1_token_type_ids = keras.Input(shape=(None,),dtype=tf.int32)\n",
    "    s1_attention_mask = keras.Input(shape=(None,),dtype=tf.int32)\n",
    "    s2_input_ids = keras.Input(shape=(None,),dtype=tf.int32)\n",
    "    s2_token_type_ids = keras.Input(shape=(None,),dtype=tf.int32)\n",
    "    s2_attention_mask = keras.Input(shape=(None,),dtype=tf.int32)\n",
    "\n",
    "    s1_embeddings = sentence_embedding_model([s1_input_ids,s1_token_type_ids,s1_attention_mask])\n",
    "    s2_embeddings = sentence_embedding_model([s2_input_ids,s2_token_type_ids,s2_attention_mask]) \n",
    "\n",
    "    scores = tf.linalg.diag_part(tf.linalg.matmul(s1_embeddings,tf.transpose(s2_embeddings))) * 10\n",
    "    scores = tf.reshape(scores,(-1,1))\n",
    "    probs = keras.layers.Dense(1,activation='sigmoid')(scores)   \n",
    "    return keras.Model([s1_input_ids,s1_token_type_ids,s1_attention_mask,s2_input_ids,s2_token_type_ids,s2_attention_mask],probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96539b58-01ef-430d-a592-aad497c26e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = create_siamese_model(sentence_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbaacaea-db9e-496a-b0cc-a08f5af0a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_opt = keras.optimizers.Adam(learning_rate=2e-5)\n",
    "bce = keras.losses.BinaryCrossentropy()\n",
    "siamese_model.compile(loss=bce, optimizer=adam_opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff7a6d-5092-4004-b136-89b50c7e889f",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f11ead7-4f09-4505-9206-6123e8a62e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_input_ids = s0_encoded['input_ids']\n",
    "s1_token_type_ids = s0_encoded['token_type_ids']\n",
    "s1_attention_mask = s0_encoded['attention_mask']\n",
    "s2_input_ids = s1_encoded['input_ids']\n",
    "s2_token_type_ids = s1_encoded['token_type_ids']\n",
    "s2_attention_mask = s1_encoded['attention_mask']\n",
    "# y_placeholder = tf.ones(len(s1_input_ids),dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dbfb2a9-b697-423a-8362-48c33f0aac9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "2146/2146 [==============================] - 535s 235ms/step - loss: 0.7168 - accuracy: 0.6901 - val_loss: 0.6532 - val_accuracy: 0.6900\n",
      "Epoch 2/5\n",
      "2146/2146 [==============================] - 494s 230ms/step - loss: 0.6320 - accuracy: 0.6913 - val_loss: 0.6418 - val_accuracy: 0.6898\n",
      "Epoch 3/5\n",
      "2146/2146 [==============================] - 495s 231ms/step - loss: 0.6174 - accuracy: 0.6920 - val_loss: 0.6247 - val_accuracy: 0.6898\n",
      "Epoch 4/5\n",
      "2146/2146 [==============================] - 498s 232ms/step - loss: 0.6039 - accuracy: 0.6930 - val_loss: 0.6351 - val_accuracy: 0.6879\n",
      "Epoch 5/5\n",
      "2146/2146 [==============================] - 498s 232ms/step - loss: 0.5844 - accuracy: 0.7021 - val_loss: 0.6466 - val_accuracy: 0.6865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f72e4a68fa0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "siamese_model.fit([s1_input_ids,s1_token_type_ids,s1_attention_mask,s2_input_ids,s2_token_type_ids,s2_attention_mask],labels,\n",
    "                  validation_data = ([dev_s1_input_ids,dev_s1_token_type_ids,dev_s1_attention_mask,dev_s2_input_ids,dev_s2_token_type_ids,dev_s2_attention_mask],dev_labels),\n",
    "                  # callbacks=[CalcAccuracyCallback((s1_input_ids[:1000],s1_token_type_ids[:1000],s1_attention_mask[:1000],s2_input_ids[:1000],s2_token_type_ids[:1000],s2_attention_mask[:1000],labels[:1000])),\n",
    "                            # CalcAccuracyCallback((dev_s1_input_ids,dev_s1_token_type_ids,dev_s1_attention_mask,dev_s2_input_ids,dev_s2_token_type_ids,dev_s2_attention_mask,dev_labels))],\n",
    "                  epochs=5,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c8cadf5-5240-474e-855b-09f79ac44799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[今天下午可能会下雨]和[今天下午可能会下雨]的相似度为： 2.0658917e-17\n",
      "[今天下午可能会下雨]和[今天天气很晴朗]的相似度为： 6.454774e-12\n",
      "[今天下午可能会下雨]和[天气预报说下午有雨]的相似度为： 2.5447382e-06\n",
      "[今天下午可能会下雨]和[北京是中国的首都]的相似度为： 5.2464917e-09\n",
      "[今天天气很晴朗]和[今天下午可能会下雨]的相似度为： 6.454774e-12\n",
      "[今天天气很晴朗]和[今天天气很晴朗]的相似度为： 2.0045364e-35\n",
      "[今天天气很晴朗]和[天气预报说下午有雨]的相似度为： 2.4299438e-09\n",
      "[今天天气很晴朗]和[北京是中国的首都]的相似度为： 1.0232005e-12\n",
      "[天气预报说下午有雨]和[今天下午可能会下雨]的相似度为： 2.5447382e-06\n",
      "[天气预报说下午有雨]和[今天天气很晴朗]的相似度为： 2.4299438e-09\n",
      "[天气预报说下午有雨]和[天气预报说下午有雨]的相似度为： 1.9866324e-12\n",
      "[天气预报说下午有雨]和[北京是中国的首都]的相似度为： 3.7954134e-08\n",
      "[北京是中国的首都]和[今天下午可能会下雨]的相似度为： 5.2464917e-09\n",
      "[北京是中国的首都]和[今天天气很晴朗]的相似度为： 1.0232005e-12\n",
      "[北京是中国的首都]和[天气预报说下午有雨]的相似度为： 3.7954134e-08\n",
      "[北京是中国的首都]和[北京是中国的首都]的相似度为： 8.345166e-35\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"今天下午可能会下雨\"\n",
    "sentence2= '今天天气很晴朗'\n",
    "sentence3= '天气预报说下午有雨'\n",
    "sentence4= '北京是中国的首都'\n",
    "\n",
    "sentence_list = [sentence1,sentence2,sentence3,sentence4]\n",
    "encoded_inputs = tokenizer(sentence_list,return_tensors='tf',padding=True)\n",
    "sentence_embeddins = sentence_embedding_model(encoded_inputs)\n",
    "for i,s1 in enumerate(sentence_list):\n",
    "    for j,s2 in enumerate(sentence_list):\n",
    "        print(f'[{s1}]和[{s2}]的相似度为：',siamese_model.predict((*tokenizer(s1,return_tensors='tf').values(),*tokenizer(s2,return_tensors='tf').values()))[0,0])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d997d4b0-0c6d-4763-ad6a-6dcdc5622dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = siamese_model.predict([dev_s1_input_ids, dev_s1_token_type_ids, dev_s1_attention_mask, dev_s2_input_ids, dev_s2_token_type_ids, dev_s2_attention_mask])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f87f0d0-d2de-49c7-b857-62ad6afdd4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集正确率 0.6865152919369787\n",
      "TP:32, TN:2931, FP:47, FN:1306\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "tp=0\n",
    "tn=0\n",
    "fp=0\n",
    "fn=0\n",
    "for score,label in zip(label_pred,dev_labels):\n",
    "    pred = True if  score>0.5 else False\n",
    "    if label == pred:\n",
    "        correct+=1\n",
    "        if label :\n",
    "            tp+=1\n",
    "        else:\n",
    "            tn+=1\n",
    "    else:\n",
    "        if pred :\n",
    "            fp+=1\n",
    "        else:\n",
    "            fn+=1\n",
    "print('测试集正确率',correct/len(dev_labels))\n",
    "print(f'TP:{tp}, TN:{tn}, FP:{fp}, FN:{fn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9cbbff-a50f-4fe2-a844-7e968ce06744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
