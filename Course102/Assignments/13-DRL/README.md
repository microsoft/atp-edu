# 课程要点：
1. 介绍游戏环境，贪食蛇何pygame基本原理，让学员可以理解game_models代码;知道如何使用pygame_utils做可视化（可视化不是模型训练必须，但是可以帮助学员理解）
2. 介绍DQN模型，configs中的一些配置的义意。
3. DQN训练困难，可以从测试曲线角度说明（训练前期准确度大升大降）








## DQN 要点
-------------------------------------------------------
1. DQN 是Q learning的一个拓展，把 Q lerning中的q-table换成了神经网络，这样能够建模更大的状态空间
2. 贝尔曼等式： 当前状态价值 
3. 双网络结构
4. Epsilon贪婪探索策略：每一步有epsilon的概率随机选择下一步的行为
5. 经验回放


## 每阶段问题

### V0.1 搭建基础DQN模型 模型无法收敛
1. 游戏状态在同一矩阵中，分别用1，2，3 代表蛇身，头 和食物。可能导致信息表达不明确
2. 使用简单全连接神经网络，按图像思路来处理的话可以使用CNN
3. 训练数据质量低，大部分是无reward的数据影
4. discount_factor的设置

### V0.1a
1. 把表示从 1，2，3 改为  64  128 255 效果不明显
2. 在只使用非0 reward训练模型时（positive-negtiave 50：50），模型的loss在不断增大（从10 到10000）
3. initial_55_memory.pkl是包含500正值500负值的训练数据


### V0.1a-1
1. 把reward改小了还是会出现loss收敛-》发散
2. initial_55_mini_reward_memory.pkl的reward缩小为1，0.01,-1


### V0.1a-2
1. 发现原来3层FC的模型很容易过拟合,改成两层FC模型就能持续一直收敛
2. 再改回三层结构依然可以收敛。说明不能收敛应该是reward过大和训练数据偏差过大的原因。（和v0.1a-1结论相悖）
3. 模型收敛后只会再图像中小范围移动，不会死也不会得分


### V0.1a-2
1. 'initial_55_mini_reward_memory2' 的正负zero更均衡，达到1：2：1
2. 'initial_55_mini_reward_memory3.pkl'中eaten的reward为1，不受下一个状态影响（这个应该是错的，因为吃一个豆后会死的话就不应该吃）
3. 'initial_55_mini_reward_memory4'基于'initial_55_mini_reward_memory2' 修改，reward为0 1 -5 -1
4. 'initial_55_mini_reward_memory5'是从头构建的数据，1 -5 -1 0

此外，尝试了如下几件事(效果都不明显)：
1. 把数据从width*width输入升级成width*width*3(头,身,食物)
2. 平衡正负例比例
3. discount_factor调大
4. 得分更改为1,-1,-5,0
5. 使用相对复杂的模型和简单模型


### V0.2
1. 基于V0.1a-2 做的一些修改
2. 'initial_55_mini_reward_memory6'重构后从头训练
3. 用v0.2在GPU机器上训练了7天（27000k step）依然是0分，310k sample。accuracy总是达不到90%
4. 标准贪食蛇启动就不是一个点，而是3多个点（有身体）；这里做了相应的修改
5. ！！！ 通过将局面缩小到5*5，模型首次有了一定的智能，最高得分达到13分，详见日志DQN-log-0207161808.log


### v0.2a
1. 将采样filter关掉(用来平衡正负例比例)，效果类似，平均可以得到10分左右，但是模型accuracy有下降，详见DQN-log-0207171732.log
2. 训练后期探索的死亡率很高，导致不能很好的继续学习新的状态（youtube视频也有提到）

### v0.2b
1. 将epsilon_decay降低到0.0002缓解死亡率高的问题。
2. 训练84w步（gpu机器12小时）之后平均得分达到12，并且有概率可以完成游戏（占满屏幕）并触发无法生成食物bug，详见0208134355(gpu)
3. 结论1，dqn可以搞定5*5的贪食蛇，但是探索速度比较慢，智能依靠更多的迭代来解决问题。
4. 模型训练到最后准确度只有0.91，说明3层线性模型能力可能已经不足。

### v0.2c  将b中的 width*height*3 输入变为  width*height，这种输入比较接近图像形式
1. 经过测试，不同类型的输入对模型的影响不明显，说明模型能自动根据输入的情况进行适应。
2. 训练22w步后测试结果约为4，详见DQN-log-0209104644.log
3. 后面将用并行程序重新进行训练

### v0.3
1. 模型可以同时使用N个arena进行训练，这样可以batch predict，提高训练效率
2. 测试也进行并行化。
3. 分离游戏逻辑和显示逻辑。
4. 一些代码优化。

### v0.3a  对比不同输入的影响：
0. v0.3系列基准： 1. DQN-0210141912.log 使用三维输入, 70w步就能达到9.7分(由于使用三维输入，模型参数增加了6k,16k->22k)。 基准2：DQN-0211120102.log, 70w步10.3分；120w步后在11.2-11.5之间波动
1. ~~int8问题BUG！！！！，DQN-log-0209182307.log 和 DQN-log-0209235016.log 使用的是 128，64，255(实际是64 -128 -1)这个方案。模型得分一直在1左右，准确度在88%，模型收敛很慢（从q_min.q_max可见）~~
2. DQN-log-0209205018.log 使用的是1，2，3方案，模型可以收敛，但是前700w步能到达7.5分，后700w步没什么提升了。
3. ~~DQN-log-0210094659.log 在2的基础上把表示scale到0.3， 0.5， 1，170w步无法收敛(原因是状态矩阵是int8，不支持小数)~~
4. DQN-log-0210102209.log 修改了3的bug，结果基本和2是一致得，所以1，2，3scale到1作用不大。
5. DQN-log-0210130433.log 方案1（代bug版本），减小lr100倍（我理解输入过大就代表模型更新梯度过大），无明显改善
6. DQN-log-0210135920.log, 修复方案1 bug，值调整为64 128 255,依然没有改善
- 总结：输入得格式对模型训练速度和效果影响很大。


### v0.3b 对比不同模型得影响（具体模型log中有记录）
1. DQN-0210141912.log，基础FC模型 22k参数,70w步到9.76分
2. DQN-0210145800.log, 5层FC，49k模型， 70w步10.4分,160w步得分11.158
3. DQN-0210153721.log, 2层全连接 9k参数，  70w 8.58,160w 11.26
4. DQN-0210161914.log, 1层全连接 2.5k参数，  70w 3分， 160w 8.4, 模型稍微差一点，收敛慢一点。
5. DQN-0210180257.log, 1层8神经元全连接，644参数， 70w 1分，但是loss很低，accuracy很高？？
6. DQN-0210181825.log, 在1的基础上，lr减小到0.0001，queue增大200w，基本没有提升，收敛速度变慢。
7, DQN-0211162651.log， 在1上换上3层cnn，250k 参数; 40w步就能到11，但是后面无法突破12
- 总结： 总体来说22k模型甚至9k模型都能达到不错的效果，调大模型没有明显提升。模型小于9k之后得分降低，但是accuracy高。

### v0.3c  epsilon对模型训练的影响
1. 依然可以使用DQN-0210141912.log和DQN-0211120102.log作为基准。
2. DQN-0211093226.log, epsilon_decay 从 2e-4 减小到2e-5, queue_size 200w. 200w步才达到8分，这时探索率10%。降低epsilon会减慢探索复杂场景速度。
3. DQN-0211105238.log, epsilon_decay 从 2e-4增大到1e-3,  收敛更快一点70w步10.27分,后续训练200步依然没有突破。
4. DQN-0211125711.log在基准的基础上把min_epsilon 从0.01改为0.001(更小的epsilon能让agent更晚的探索？但是同时探索概率也减小了),500w步无法突破12，没什么进步
5. DQN-0211144956.log 在基准的基础上把min_epsilon 从0.01改为0.1,也是能达到11分多，但是得分波动更大。
总结: epsilon会影响训练速度，但对结果影响不大。


### v0.4
1. DQN-0211181058.log, 加入死循环跳出后训练结果没有影响
2. DQN-0211193739.log, 修改了eaten后得分计算规则bug，得分突破12分，1000w步达到12分，感觉任务变难了
3. ** v0.4基准代码，DQN-0212140626.log  修复bug基础上使用cnn模型(valid padding), 150w步达到13分  1000w步达到接近14分
4. DQN-0212184032.log, 在3的基础上DQN_DISCOUNT_FACTOR增大到0.99(贪食蛇只重视长期价值，只要不是无限循环就行)，800w步达到18分，但是曲线奇怪，下面再跑一次
5. DQN-0212222711.log, 和上面4一样的配置,和上面结果一致，得分曲线很奇怪？？
6. DQN-0213090421.log, 在4,5配置上死亡得分减到-20，效果有一点下降，得分曲线更奇怪了。
7. DQN-0213161359.log, 在2的基础上discount-factor增大到0.99，还是最大大约12分。(FC模型)
8. DQN-0213203746.log, 和3，4类似，但是选用same padding 1,3,3，参数从7w到22w，但结果没有提升，反而在后段会下降，有点过拟合的感觉。


### v0.4a
1. 0.4基础上分析模型发现在极难情况下错误走法（倒退）的q值反而不够小，影响了模型判断。
2. 1200w步，后期的状况训练不充分，有些状况明显是没训练到才出现错误.
3. DQN-0214132843.log, v0.4代码，invalid_command reward改为-5,得分能达到18以上（1000w步），和上面的DQN-0212184032.log类似，但比下面的DQN-0214211155.log更高（理论上这两个应该是一样的代码）
4. DQN-0214211155.log, v0.4原版代码,不做修改，17分多，不到18。（这里多了starving stop，影响性能） 再跑一次DQN-0215102425.log。两次训练结果完全相同！(DQN-0215132844.log 这是在GPU上跑的，结果和cpu有些许差异)
5. DQN-0215103337.log, 和4相同，但是random_seed为261，300w步到16。不同随机数没有太大区别。


### v0.4b  删掉了hunger stop，test 函数加入staring状态使结果更精准
0. DQN-0216103938.log, 基准测试(错误指令为-1)，理论上应该和DQN-0212184032.log结论类似;   结果曲线不一样，但是最大值能600w步到18分，和前面的基准类似。
1. DQN-0215155548.log, 错误指令score变为-5, 最高接近20，继续训练会掉到17-18, starving数接近0， invalid_step更多
2. DQN-0215181451.log, 错误指令score变为-10(宁可死也不能无效行为),最高达到20，但是训练过久会掉到17-18，starving多，invalid数少
- 结论，-5，-10影响不大，目前选择-5


### v0.4c, 在v04b修改 invalid command 为-5的基础上试验概率探索和区分首尾
1. 基准为DQN-0215155548.log
2. DQN-0216154019.log, 区分首尾，从尾巴到脖子依次+1,比基准略微好了一点点，但是训练速度更慢
3. DQN-0216163320.log, 使用概率action探索代替e-greedy, 收敛慢（1000w步15分，6000w步最高17.5），效果差(探索的不够均匀，好像无法满足q-learning的假设)
4. DQN-0217095300.log, 从模型"/home/vai/hchan/pySnake/models/0216154019/22000000m.h5"(已经是一个19分的模型)开始概率探索.效果也不好
5. DQN-0218161901.log, 10*10 arena   能达到50分(总共跑了两天，前面到40分非常快，从40到50慢很多)
 

### v0.5   目前最好成绩，21.4分
1. DQN-0303135223.log, 在DQN-0215155548.log的基础上，重新考虑invalid-step，把他当作valid-step,但是下一状态等于当前状态。训练速度略慢但效果好，测试最高达到21.47分(1210w步)

### todo
- [x] 对比三维输入和二维输入：v0.2c； v0.3abc；  为什么观测值使用大数无法收敛，有待进一步研究。
- [x] 模型测试得分可视化
- [ ] learning rate 减小，decay
- [x] 消融实验,不同复杂度得模型。见v0.3b
- [x] 消融实验，不同queue size. 见v0.3b-6.效果不明显
- [x] CNN模型效果,v03b收敛更快，受限与eaten得分bug无法突破12分，
- [x] hunger_steps,提前结束陷入循环的游戏, hunger_step较难设定，设定了不好的hunger_step 反而影响模型效果（影响探索）
- [ ] eaten后得分是随机的（和新食物位置有关）
- [ ] obs信息不完备，无法判断蛇的路线
- [ ] 模型loss下不去，准确度上不来，即使使用复杂模型也无法“过拟合”，这可能和强化学习的样例采样有关。
- [x] 使用不同随机数种子训练曲线是否会不同，比如准确度中间的大坑。 见v0.4a,不同随机数区别不太大
- [ ] Q-learning step decay(保证收敛)

### 资料
1. 标准贪食蛇初始时长度不为1（一般是3），只有三个action，不能倒退.
2. 最基础的启蒙教程，使用简易状态+DQN。https://towardsdatascience.com/how-to-teach-an-ai-to-play-games-deep-reinforcement-learning-28f9b920440a
3. 一个基础简洁的DQN实现 https://github.com/mswang12/minDQN
4. 关于贪食蛇AI的高级介绍，看了这个视频（https://www.youtube.com/watch?v=i0Pkgtbh1xw）发现20*20的游戏并不简单。 他使用了CNN+FC模型，输入为多帧图像.
视频说明了epsilon greedy的探索方式在贪食蛇中存在容易死掉的问题，降低了探索效率。视频中说6 * 6的游戏通常要200步能胜利。视频中使用greedy epsilon在10*10中最多可以得50分。
使用actor-critic 算法更好，可以完美解决6*6游戏，该算法会更多向比较好得方向探索。 10*10训练4小时后可以有概率获胜. 20*20 18小时能得大约80分.
后续可以尝试PPO或者蒙特卡洛树搜索
3. 贪食蛇有完美解,奇数的可以通过漏过一点变换路径，偶数的可以按固定路径走。

# RL的一些记录
1. 对于RL，reward稀疏或滞后的任务比较难做，
2. sparse reward 问题

# tf 环境按照
1. 安装conda，创建新的py3 env
2. (optional) 安装nvidia driver
3. conda install cudnn(will install cuda and cudnn)
4. pip install tensorflow-gpu
5. conda env config vars set LD_LIBRARY_PATH=$CONDA_PREFIX/lib  (tensorflow bug:https://github.com/tensorflow/tensorflow/issues/52988 )