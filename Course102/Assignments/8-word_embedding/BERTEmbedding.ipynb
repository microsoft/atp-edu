{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db7a033-5a58-47b5-aff7-7a845e4f0191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hchan/miniconda3/envs/allinone/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,TFAutoModel\n",
    "import tensorflow as tf\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097b731-053f-42c9-9980-acf457b9053a",
   "metadata": {},
   "source": [
    "## 本实验使用transformers库来进行\n",
    "https://github.com/huggingface/transformers\n",
    "\n",
    "https://huggingface.co/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da150df6-7e03-4649-82bb-cb9466cf503d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-chinese were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = TFAutoModel.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa7ffe22-71ba-4f25-a37e-d14310a9e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如何使用transformers从本地读取模型\n",
    "# save model locally and load model from local file system.\n",
    "# config.save_pretrained('/data/hchan/huggingface-bert-base-chinese')\n",
    "# tokenizer.save_pretrained('/data/hchan/huggingface-bert-base-chinese')\n",
    "# model.save_pretrained('/data/hchan/huggingface-bert-base-chinese')\n",
    "# config = AutoConfig.from_pretrained('/data/hchan/huggingface-bert-base-chinese')\n",
    "# tokenizer = BertTokenizer.from_pretrained('/data/hchan/huggingface-bert-base-chinese')\n",
    "# model = TFAutoModel.from_pretrained('/data/hchan/huggingface-bert-base-chinese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bda4d5-72c4-4039-a176-10ef13b1b89f",
   "metadata": {},
   "source": [
    "### Bert 模型输入的一些规则\n",
    "1. 句首加[cls],每个句子句尾加[sep]\n",
    "2. token_type_ids 用来区分多个句子的情况\n",
    "3. attention_mask 用来区别句子内容和padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d1c20b6-9217-44eb-9984-02a581d35c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized text: ['今', '天', '我', '们', '来', '学', '习', '[UNK]', '的', '用', '法']\n",
      "encoded_tensor: {'input_ids': [101, 791, 1921, 2769, 812, 3341, 2110, 739, 100, 4638, 4500, 3791, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "decoded model input: [CLS] 今 天 我 们 来 学 习 [UNK] 的 用 法 [SEP]\n"
     ]
    }
   ],
   "source": [
    "text = \"今天我们来学习BERT的用法\"\n",
    "print('tokenized text:',tokenizer.tokenize(text))\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "print('encoded_tensor:',tokenizer(text))\n",
    "print(\"decoded model input:\",tokenizer.decode(tokenizer(text)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59aad071-e345-4cdb-b58d-53b402f1e73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 13, 768), dtype=float32, numpy=\n",
       "array([[[ 0.55845565, -0.2177088 , -1.1836438 , ...,  0.44294152,\n",
       "          0.4910655 ,  0.37872693],\n",
       "        [-0.16212857, -0.22858876,  0.22160313, ..., -1.3752859 ,\n",
       "         -0.19599125, -0.13314222],\n",
       "        [ 0.59043   , -0.6807966 , -1.5145776 , ..., -0.38338068,\n",
       "          1.0174727 ,  0.23099777],\n",
       "        ...,\n",
       "        [ 0.33431968, -0.06720637,  0.05515486, ...,  0.04679219,\n",
       "          0.402681  ,  0.09251919],\n",
       "        [ 0.5097357 , -0.8889353 , -0.5622668 , ..., -0.56882447,\n",
       "          0.42690647,  0.39236704],\n",
       "        [-0.43672717, -0.31467965, -1.1357595 , ..., -0.86004853,\n",
       "          0.45863438, -0.09646266]]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(encoded_input)\n",
    "output.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459c2148-48ea-46f7-9ee9-2aee2512a5f8",
   "metadata": {},
   "source": [
    "## BERT词嵌入是和上下文有关的，对比三种语境下的中国和美国的差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374c6c37-aed5-4911-bb87-1896a89bb506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=122.12986>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1='中国是个发展中国家'\n",
    "output1 = model(tokenizer(text1, return_tensors='tf'))\n",
    "embedding_china = tf.math.reduce_mean(output1.last_hidden_state[0,1:3,:],axis=0)\n",
    "text2='美国是个发达的国家'\n",
    "output2 = model(tokenizer(text2, return_tensors='tf'))\n",
    "embedding_us = tf.math.reduce_mean(output2.last_hidden_state[0,1:3,:],axis=0)\n",
    "tf.math.reduce_sum(tf.math.square(embedding_china-embedding_us))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69b66c7f-d797-40bb-a958-312228ac9540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=191.0534>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1='中国是个强大的国家'\n",
    "output1 = model(tokenizer(text1, return_tensors='tf'))\n",
    "embedding_china = tf.math.reduce_mean(output1.last_hidden_state[0,1:3,:],axis=0)\n",
    "text2='美国大学在世界上排名领先'\n",
    "output2 = model(tokenizer(text2, return_tensors='tf'))\n",
    "embedding_us = tf.math.reduce_mean(output2.last_hidden_state[0,1:3,:],axis=0)\n",
    "tf.math.reduce_sum(tf.math.square(embedding_china-embedding_us))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4422850-9756-42e5-afdf-2bf4fa16a48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=272.4551>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1='中国是个强大的国家'\n",
    "output1 = model(tokenizer(text1, return_tensors='tf'))\n",
    "embedding_china = tf.math.reduce_mean(output1.last_hidden_state[0,1:3,:],axis=0)\n",
    "text2='世界上没有完美国家'\n",
    "output2 = model(tokenizer(text2, return_tensors='tf'))\n",
    "embedding_us = tf.math.reduce_mean(output2.last_hidden_state[0,7:9,:],axis=0)\n",
    "tf.math.reduce_sum(tf.math.square(embedding_china-embedding_us))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f4f19-ac59-4131-844c-2be4ad799d80",
   "metadata": {},
   "source": [
    "# 额外内容，用词嵌入的平均值计算句子相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1c1a96b-8b97-46fd-93d5-18e3fd83e21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(4, 11), dtype=int32, numpy=\n",
       "array([[ 101,  791, 1921,  678, 1286, 1377, 5543,  833,  678, 7433,  102],\n",
       "       [ 101,  791, 1921, 1921, 3698, 2523, 3252, 3306,  102,    0,    0],\n",
       "       [ 101, 1921, 3698, 7564, 2845, 6432,  678, 1286, 3300, 7433,  102],\n",
       "       [ 101, 1266,  776, 3221,  704, 1744, 4638, 7674, 6963,  102,    0]],\n",
       "      dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(4, 11), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(4, 11), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = \"今天下午可能会下雨\"\n",
    "sentence2= '今天天气很晴朗'\n",
    "sentence3= '天气预报说下午有雨'\n",
    "sentence4= '北京是中国的首都'\n",
    "\n",
    "sentence_list = [sentence1,sentence2,sentence3,sentence4]\n",
    "encoded_inputs = tokenizer(sentence_list,return_tensors='tf',padding=True)\n",
    "encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c904da0b-9eb6-4d1c-b4aa-24d4f03419e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[今天下午可能会下雨]和[今天下午可能会下雨]的L2距离为： 0.0\n",
      "[今天下午可能会下雨]和[今天天气很晴朗]的L2距离为： 97.23857\n",
      "[今天下午可能会下雨]和[天气预报说下午有雨]的L2距离为： 54.660233\n",
      "[今天下午可能会下雨]和[北京是中国的首都]的L2距离为： 185.97879\n",
      "[今天天气很晴朗]和[今天下午可能会下雨]的L2距离为： 97.23857\n",
      "[今天天气很晴朗]和[今天天气很晴朗]的L2距离为： 0.0\n",
      "[今天天气很晴朗]和[天气预报说下午有雨]的L2距离为： 91.68937\n",
      "[今天天气很晴朗]和[北京是中国的首都]的L2距离为： 199.58102\n",
      "[天气预报说下午有雨]和[今天下午可能会下雨]的L2距离为： 54.660233\n",
      "[天气预报说下午有雨]和[今天天气很晴朗]的L2距离为： 91.68937\n",
      "[天气预报说下午有雨]和[天气预报说下午有雨]的L2距离为： 0.0\n",
      "[天气预报说下午有雨]和[北京是中国的首都]的L2距离为： 184.15384\n",
      "[北京是中国的首都]和[今天下午可能会下雨]的L2距离为： 185.97879\n",
      "[北京是中国的首都]和[今天天气很晴朗]的L2距离为： 199.58102\n",
      "[北京是中国的首都]和[天气预报说下午有雨]的L2距离为： 184.15384\n",
      "[北京是中国的首都]和[北京是中国的首都]的L2距离为： 0.0\n"
     ]
    }
   ],
   "source": [
    "sentence_embeddins = tf.math.reduce_mean(model(encoded_inputs).last_hidden_state,axis=1)\n",
    "for i,s1 in enumerate(sentence_list):\n",
    "    for j,s2 in enumerate(sentence_list):\n",
    "        print(f'[{s1}]和[{s2}]的L2距离为：',tf.math.reduce_sum(tf.math.square(sentence_embeddins[i]-sentence_embeddins[j])).numpy())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef205a2d-e863-46d1-b196-7f197cdf1ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
